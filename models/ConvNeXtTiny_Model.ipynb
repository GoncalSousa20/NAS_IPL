{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "# Define the paths to your three folders\n",
        "train_path = '/content/drive/MyDrive/ProjetoCurso/train'\n",
        "validation_path = '/content/drive/MyDrive/ProjetoCurso/validation'\n",
        "test_path = '/content/drive/MyDrive/ProjetoCurso/test'\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "# Augment data\n",
        "batch_size = 16\n",
        "train_input_shape = (224, 224, 3)\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1./255.,\n",
        "                                   #rotation_range=45,\n",
        "                                   #width_shift_range=0.5,\n",
        "                                   #height_shift_range=0.5,\n",
        "                                   shear_range=5,\n",
        "                                   #zoom_range=0.7,\n",
        "                                   horizontal_flip=True,\n",
        "                                   vertical_flip=True,\n",
        "                                  )\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(directory='/content/drive/MyDrive/ProjetoCurso/train',\n",
        "                                                    class_mode='categorical',\n",
        "                                                    target_size=train_input_shape[0:2],\n",
        "                                                    batch_size=batch_size,\n",
        "                                                    shuffle=True,\n",
        "                                                   )\n",
        "\n",
        "valid_generator = train_datagen.flow_from_directory(directory='/content/drive/MyDrive/ProjetoCurso/validation',\n",
        "                                                    class_mode='categorical',\n",
        "                                                    target_size=train_input_shape[0:2],\n",
        "                                                    batch_size=batch_size,\n",
        "                                                    shuffle=True,\n",
        "                                                   )\n",
        "\n",
        "test_generator = train_datagen.flow_from_directory(directory='/content/drive/MyDrive/ProjetoCurso/test',\n",
        "                                                    class_mode='categorical',\n",
        "                                                    target_size=train_input_shape[0:2],\n",
        "                                                    batch_size=1,\n",
        "                                                    shuffle=False,\n",
        "                                                   )\n",
        "\n",
        "STEP_SIZE_TRAIN = train_generator.n//train_generator.batch_size\n",
        "STEP_SIZE_VALID = valid_generator.n//valid_generator.batch_size\n",
        "STEP_SIZE_TEST = test_generator.n//test_generator.batch_size\n",
        "print(\"Total number of batches =\", STEP_SIZE_TRAIN, \"and\", STEP_SIZE_VALID, \"and\", STEP_SIZE_TEST)\n",
        "\n",
        "from keras.applications import ConvNeXtTiny\n",
        "base_model = ConvNeXtTiny(weights='imagenet', include_top=False, input_shape=train_input_shape)\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "from keras.layers import Dense, Flatten, BatchNormalization, Activation, Dropout\n",
        "from keras.models import Model\n",
        "\n",
        "n_classes = 11\n",
        "\n",
        "# Add layers at the end\n",
        "X = base_model.output\n",
        "X = Flatten()(X)\n",
        "\n",
        "# First dense block\n",
        "X = Dense(512, kernel_initializer='he_uniform')(X)\n",
        "X = BatchNormalization()(X)\n",
        "X = Activation('relu')(X)\n",
        "# Optionally add dropout for regularization\n",
        "# X = Dropout(0.5)(X)\n",
        "\n",
        "# Second dense block\n",
        "X = Dense(256, kernel_initializer='he_uniform')(X)\n",
        "X = BatchNormalization()(X)\n",
        "X = Activation('relu')(X)\n",
        "# Optionally add dropout for regularization\n",
        "# X = Dropout(0.5)(X)\n",
        "\n",
        "# Third dense block\n",
        "X = Dense(128, kernel_initializer='he_uniform')(X)\n",
        "X = BatchNormalization()(X)\n",
        "X = Activation('relu')(X)\n",
        "# Optionally add dropout for regularization\n",
        "# X = Dropout(0.5)(X)\n",
        "\n",
        "# Fourth dense block\n",
        "X = Dense(64, kernel_initializer='he_uniform')(X)\n",
        "X = BatchNormalization()(X)\n",
        "X = Activation('relu')(X)\n",
        "# Optionally add dropout for regularization\n",
        "# X = Dropout(0.5)(X)\n",
        "\n",
        "# Fifth dense block\n",
        "X = Dense(32, kernel_initializer='he_uniform')(X)\n",
        "X = BatchNormalization()(X)\n",
        "X = Activation('relu')(X)\n",
        "# Optionally add dropout for regularization\n",
        "# X = Dropout(0.5)(X)\n",
        "\n",
        "# Final dense layer before output\n",
        "X = Dense(16, kernel_initializer='he_uniform')(X)\n",
        "X = BatchNormalization()(X)\n",
        "X = Activation('relu')(X)\n",
        "# Optionally add dropout for regularization\n",
        "# X = Dropout(0.5)(X)\n",
        "\n",
        "# Output layer\n",
        "output = Dense(n_classes, activation='softmax')(X)\n",
        "\n",
        "# Create the model\n",
        "model = Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "from keras.optimizers import Adam\n",
        "optimizer = Adam(lr=0.0001)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    matthews_corrcoef, cohen_kappa_score, hamming_loss, zero_one_loss, roc_auc_score,\n",
        "    average_precision_score, brier_score_loss, classification_report\n",
        ")\n",
        "import pickle\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "seed = 42\n",
        "tf.random.set_seed(seed)\n",
        "np.random.seed(seed)\n",
        "# Additional custom metrics\n",
        "def fbeta_score(precision, recall, beta=1.0):\n",
        "    if precision + recall == 0:\n",
        "        return 0.0\n",
        "    return (1 + beta*2) * (precision * recall) / (beta*2 * precision + recall)\n",
        "\n",
        "def g_mean(precision, recall):\n",
        "    return np.sqrt(precision * recall)\n",
        "\n",
        "# Example usage\n",
        "dictionary_metrics = {\n",
        "    'train_acc': [],\n",
        "    'train_loss': [],\n",
        "    'precision': [],\n",
        "    'recall': [],\n",
        "    'val_loss': [],\n",
        "    'test_acc': [],\n",
        "    'test_loss': [],\n",
        "    'val_accuracy': [],\n",
        "    'f1_score': [],\n",
        "    'mcc': [],\n",
        "    'cohen_kappa': [],\n",
        "    'hamming_loss': [],\n",
        "    'zero_one_loss': [],\n",
        "    'auc_roc': [],\n",
        "    'brier_score': [],\n",
        "    'mAP': [],\n",
        "    'f1_beta': [],\n",
        "    'g_mean': []\n",
        "}\n",
        "\n",
        "# Function to get predictions and labels in batches\n",
        "def get_predictions_and_labels(generator, model, steps):\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    y_pred_prob = []\n",
        "\n",
        "    for i in range(steps):\n",
        "        (X, y) = next(generator)\n",
        "        y_pred_prob_batch = model.predict(X)\n",
        "        y_pred_batch = np.argmax(y_pred_prob_batch, axis=1)\n",
        "\n",
        "        y_pred.append(y_pred_batch)\n",
        "        y_pred_prob.append(y_pred_prob_batch)\n",
        "        y_true.append(y)\n",
        "\n",
        "    # Flatten the lists\n",
        "    y_pred = np.concatenate(y_pred, axis=0)\n",
        "    y_pred_prob = np.concatenate(y_pred_prob, axis=0)\n",
        "    y_true = np.concatenate(y_true, axis=0)\n",
        "\n",
        "    # Update Truth vector based on argmax if y is one-hot encoded\n",
        "    if y_true.ndim > 1 and y_true.shape[1] > 1:\n",
        "        y_true = np.argmax(y_true, axis=1)\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    y_pred = np.asarray(y_pred).ravel()\n",
        "    y_pred_prob = np.asarray(y_pred_prob)\n",
        "\n",
        "    return y_true, y_pred,y_pred_prob"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FuDWB-20TvF",
        "outputId": "ad164076-c356-42f8-c849-37d4648836eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Found 3862 images belonging to 11 classes.\n",
            "Found 216 images belonging to 11 classes.\n",
            "Found 219 images belonging to 11 classes.\n",
            "Total number of batches = 241 and 13 and 219\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/convnext/convnext_tiny_notop.h5\n",
            "111650432/111650432 [==============================] - 1s 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(30):\n",
        "  print(f\"Starting Epoch {epoch+1}\")\n",
        "  history = model.fit(\n",
        "      train_generator,\n",
        "      validation_data=valid_generator,\n",
        "      epochs=1\n",
        "  )\n",
        "  print(history.history)\n",
        "  print(f\"Finished Epoch {epoch+1}\")\n",
        "\n",
        "  # Extract validation labels\n",
        "  val_labels = []\n",
        "  for i in range(len(valid_generator)):\n",
        "      _, batch_labels = next(valid_generator)\n",
        "      val_labels.extend(batch_labels)\n",
        "\n",
        "  val_labels = np.array(val_labels)\n",
        "  dictionary_metrics['train_acc'].append(history.history[\"accuracy\"])\n",
        "  dictionary_metrics['train_loss'].append(history.history[\"loss\"])\n",
        "\n",
        "  # Calculate metrics for validation and test sets\n",
        "  val_loss, val_accuracy = model.evaluate(valid_generator)\n",
        "  dictionary_metrics['val_accuracy'].append(val_accuracy)\n",
        "  dictionary_metrics['val_loss'].append(val_loss)\n",
        "  test_loss, test_acc = model.evaluate(test_generator)\n",
        "  dictionary_metrics['test_loss'].append(test_loss)\n",
        "  dictionary_metrics['test_acc'].append(test_acc)\n",
        "\n",
        "  test_labels, test_pred_labels, test_pred_prob = get_predictions_and_labels(test_generator, model, steps=STEP_SIZE_TEST)\n",
        "\n",
        "  num_classes = len(np.unique(test_labels))\n",
        "\n",
        "  test_classification_report = classification_report(test_labels, test_pred_labels, output_dict=True)\n",
        "  test_recall_list = []\n",
        "  test_precision_list = []\n",
        "  test_f1_score_list = []\n",
        "  test_f1_beta_list = []\n",
        "  test_g_mean_list = []\n",
        "\n",
        "  for class_label in range(num_classes):\n",
        "      recall = test_classification_report[str(class_label)]['recall']\n",
        "      precision = test_classification_report[str(class_label)]['precision']\n",
        "      f1 = test_classification_report[str(class_label)]['f1-score']\n",
        "      test_recall_list.append(recall)\n",
        "      test_precision_list.append(precision)\n",
        "      test_f1_score_list.append(f1)\n",
        "      test_f1_beta_list.append(fbeta_score(precision, recall, beta=2.0))  # Using beta=2 for F1-beta score\n",
        "      test_g_mean_list.append(g_mean(precision, recall))\n",
        "\n",
        "  dictionary_metrics['recall'].append(test_recall_list)\n",
        "  dictionary_metrics['precision'].append(test_precision_list)\n",
        "  dictionary_metrics['f1_score'].append(test_f1_score_list)\n",
        "  dictionary_metrics['f1_beta'].append(test_f1_beta_list)\n",
        "  dictionary_metrics['g_mean'].append(test_g_mean_list)\n",
        "  dictionary_metrics['mcc'].append(matthews_corrcoef(test_labels, test_pred_labels))\n",
        "  dictionary_metrics['cohen_kappa'].append(cohen_kappa_score(test_labels, test_pred_labels))\n",
        "  dictionary_metrics['hamming_loss'].append(hamming_loss(test_labels, test_pred_labels))\n",
        "  dictionary_metrics['zero_one_loss'].append(zero_one_loss(test_labels, test_pred_labels, normalize=True))\n",
        "\n",
        "  try:\n",
        "      test_roc_auc = roc_auc_score(test_labels, test_pred_prob, multi_class='ovr')\n",
        "      dictionary_metrics['auc_roc'].append(test_roc_auc)\n",
        "  except ValueError:\n",
        "      print(\"Error in calculating ROC AUC for test set\")\n",
        "\n",
        "  try:\n",
        "      avg_precision = []\n",
        "      for i in range(num_classes):\n",
        "          avg_precision.append(average_precision_score((test_labels == i).astype(int), test_pred_prob[:, i]))\n",
        "      avg_precision = np.mean(avg_precision)\n",
        "      dictionary_metrics['mAP'].append(avg_precision)\n",
        "  except ValueError as e:\n",
        "      print(f\"Error in calculating Average Precision: {e}\")\n",
        "\n",
        "  # Calculate Brier score for each class and average them\n",
        "  brier_scores = []\n",
        "  for i in range(num_classes):\n",
        "      true_binary = (test_labels == i).astype(float)\n",
        "      probas = test_pred_prob[:, i]\n",
        "      brier_score = brier_score_loss(true_binary, probas)\n",
        "      brier_scores.append(brier_score)\n",
        "\n",
        "  average_brier_score = np.mean(brier_scores)\n",
        "  dictionary_metrics['brier_score'].append(average_brier_score)\n",
        "\n",
        "with open('/content/drive/MyDrive/ProjetoCurso/metrics_ConvNeXtTiny.pkl', 'wb') as fp:\n",
        "    pickle.dump(dictionary_metrics,fp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "io1g_bqA0eJ5",
        "outputId": "21623a13-1a37-4380-b0e1-02f63c32e012"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Epoch 1\n",
            "  3/242 [..............................] - ETA: 2:18 - loss: 0.4826 - accuracy: 0.7917"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-345c3cea6c58>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Starting Epoch {epoch+1}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   history = model.fit(\n\u001b[0m\u001b[1;32m      4\u001b[0m       \u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1805\u001b[0m                         ):\n\u001b[1;32m   1806\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1807\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1808\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    869\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "imI5W7Y7cVb1",
        "outputId": "7fbfe088-e50e-404b-e5cb-7849716b75e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Function to print the metrics in an organized way\n",
        "def print_metrics(metrics):\n",
        "    print(\"Metrics:\")\n",
        "    print(\"--------\")\n",
        "    for key, value in metrics.items():\n",
        "        if isinstance(value, list):  # Check if the value is a list (e.g., confusion matrix)\n",
        "            print(f\"{key}:\")\n",
        "            for row in value:\n",
        "                print(f\"  {row}\")\n",
        "        else:\n",
        "            print(f\"{key}: {value}\")\n",
        "\n",
        "# Reading the metrics from the pickle file\n",
        "try:\n",
        "    with open('/content/drive/MyDrive/ProjetoCurso/metrics_ConvNeXtTiny.pkl', 'rb') as fp:\n",
        "        loaded_metrics = pickle.load(fp)\n",
        "    print(\"Metrics successfully loaded from metrics.pkl\")\n",
        "    print_metrics(loaded_metrics)\n",
        "except EOFError:\n",
        "    print(\"Error: Ran out of input - the file may be empty or improperly written.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: The file does not exist.\")\n",
        "except pickle.UnpicklingError:\n",
        "    print(\"Error: The file contents are not a valid pickle.\")\n"
      ],
      "metadata": {
        "id": "ymbdz7q_scMH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43caad82-c9a2-4ab6-de27-7b8e638a93b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics successfully loaded from metrics.pkl\n",
            "Metrics:\n",
            "--------\n",
            "train_acc:\n",
            "  [0.24054893851280212]\n",
            "  [0.34955981373786926]\n",
            "  [0.41222164034843445]\n",
            "  [0.4554634988307953]\n",
            "  [0.48083895444869995]\n",
            "  [0.5113930702209473]\n",
            "  [0.5450543761253357]\n",
            "  [0.5743138194084167]\n",
            "  [0.5792335867881775]\n",
            "  [0.5921801924705505]\n",
            "  [0.6121180653572083]\n",
            "  [0.6408596634864807]\n",
            "  [0.6431900858879089]\n",
            "  [0.6535474061965942]\n",
            "  [0.6794406771659851]\n",
            "  [0.6952356100082397]\n",
            "  [0.6892801523208618]\n",
            "  [0.696530282497406]\n",
            "  [0.7084411978721619]\n",
            "  [0.722682535648346]\n",
            "  [0.7330398559570312]\n",
            "  [0.7452097535133362]\n",
            "  [0.7421025633811951]\n",
            "  [0.7679958343505859]\n",
            "  [0.7734334468841553]\n",
            "  [0.7716209292411804]\n",
            "  [0.7788710594177246]\n",
            "  [0.7752459645271301]\n",
            "  [0.7806835770606995]\n",
            "  [0.7964785099029541]\n",
            "train_loss:\n",
            "  [2.203213691711426]\n",
            "  [1.9452670812606812]\n",
            "  [1.781105875968933]\n",
            "  [1.658245325088501]\n",
            "  [1.5650873184204102]\n",
            "  [1.4679745435714722]\n",
            "  [1.3983641862869263]\n",
            "  [1.3224818706512451]\n",
            "  [1.2744336128234863]\n",
            "  [1.236789345741272]\n",
            "  [1.1612224578857422]\n",
            "  [1.116363525390625]\n",
            "  [1.102885365486145]\n",
            "  [1.0477176904678345]\n",
            "  [1.0017141103744507]\n",
            "  [0.9500369429588318]\n",
            "  [0.9551658630371094]\n",
            "  [0.9061740636825562]\n",
            "  [0.8958210945129395]\n",
            "  [0.8577038645744324]\n",
            "  [0.8295857310295105]\n",
            "  [0.7870039343833923]\n",
            "  [0.8011170029640198]\n",
            "  [0.72728431224823]\n",
            "  [0.7200589776039124]\n",
            "  [0.7236034870147705]\n",
            "  [0.6898356676101685]\n",
            "  [0.6896781921386719]\n",
            "  [0.6734286546707153]\n",
            "  [0.6324161887168884]\n",
            "precision:\n",
            "  [0.0, 0.0, 0.31645569620253167, 0.0, 0.5, 0.6, 0.0, 0.2222222222222222, 0.2222222222222222, 0.75, 0.42105263157894735]\n",
            "  [0.30434782608695654, 0.0, 0.2413793103448276, 0.4583333333333333, 0.6666666666666666, 0.5882352941176471, 0.0, 0.0, 0.6666666666666666, 0.0, 0.46551724137931033]\n",
            "  [0.16129032258064516, 0.0, 0.7777777777777778, 0.6428571428571429, 0.875, 0.55, 0.0, 0.0, 0.2222222222222222, 0.0, 0.42424242424242425]\n",
            "  [0.7058823529411765, 0.5, 0.3230769230769231, 0.6, 0.5555555555555556, 0.48148148148148145, 0.0, 0.3333333333333333, 0.5, 0.5, 0.6341463414634146]\n",
            "  [0.6, 0.07142857142857142, 0.6666666666666666, 0.6, 0.42857142857142855, 0.8333333333333334, 0.0, 0.0, 0.0, 0.0, 0.22413793103448276]\n",
            "  [1.0, 0.25, 0.34210526315789475, 0.6666666666666666, 0.7777777777777778, 0.6666666666666666, 0.16666666666666666, 0.22580645161290322, 0.0, 0.5, 0.46938775510204084]\n",
            "  [0.27419354838709675, 0.35294117647058826, 0.6296296296296297, 0.47368421052631576, 0.625, 0.48, 0.0, 0.3333333333333333, 0.5, 0.3333333333333333, 0.49056603773584906]\n",
            "  [0.0, 0.6666666666666666, 0.4666666666666667, 0.75, 0.5384615384615384, 0.5, 0.0, 0.1951219512195122, 0.4166666666666667, 0.38461538461538464, 0.3918918918918919]\n",
            "  [0.3617021276595745, 0.21739130434782608, 0.5227272727272727, 0.47368421052631576, 0.7, 0.7, 0.0, 0.375, 0.3055555555555556, 0.2222222222222222, 0.6153846153846154]\n",
            "  [0.6956521739130435, 0.6666666666666666, 0.43661971830985913, 0.8333333333333334, 0.7, 0.6875, 0.5714285714285714, 0.6, 0.5, 0.4666666666666667, 0.65]\n",
            "  [0.4411764705882353, 0.32142857142857145, 0.4594594594594595, 0.6363636363636364, 0.6666666666666666, 0.625, 0.25, 0.3333333333333333, 1.0, 1.0, 0.6]\n",
            "  [0.7142857142857143, 0.4, 0.5454545454545454, 0.4782608695652174, 1.0, 0.5416666666666666, 0.0, 0.3548387096774194, 0.42857142857142855, 0.6363636363636364, 0.5555555555555556]\n",
            "  [0.6923076923076923, 0.21818181818181817, 0.6363636363636364, 1.0, 1.0, 0.4186046511627907, 0.5, 0.42857142857142855, 0.6, 0.6, 0.7096774193548387]\n",
            "  [0.8333333333333334, 0.6, 0.32967032967032966, 0.47368421052631576, 0.4, 0.9090909090909091, 0.0, 0.0, 0.45454545454545453, 0.3333333333333333, 0.5384615384615384]\n",
            "  [1.0, 0.4666666666666667, 0.7692307692307693, 0.6666666666666666, 0.875, 0.5, 0.4, 0.1590909090909091, 0.2727272727272727, 0.5, 0.5675675675675675]\n",
            "  [0.75, 0.45454545454545453, 0.3559322033898305, 0.6923076923076923, 0.75, 0.6923076923076923, 0.3333333333333333, 0.2, 0.5, 0.6666666666666666, 0.5833333333333334]\n",
            "  [0.9, 0.5833333333333334, 0.5714285714285714, 0.7142857142857143, 1.0, 0.6666666666666666, 0.4, 0.3333333333333333, 0.5, 0.42857142857142855, 0.4918032786885246]\n",
            "  [1.0, 0.46153846153846156, 0.7368421052631579, 0.7, 0.6153846153846154, 0.875, 0.23529411764705882, 0.20454545454545456, 0.3448275862068966, 0.2962962962962963, 0.53125]\n",
            "  [0.2807017543859649, 0.5, 0.5714285714285714, 0.5454545454545454, 0.7142857142857143, 0.5217391304347826, 0.3333333333333333, 0.4, 0.5555555555555556, 0.4444444444444444, 0.6571428571428571]\n",
            "  [0.46875, 0.8571428571428571, 1.0, 0.4, 0.8571428571428571, 0.5909090909090909, 0.26666666666666666, 0.25, 0.5714285714285714, 0.37037037037037035, 0.5714285714285714]\n",
            "  [0.7777777777777778, 0.22448979591836735, 0.5853658536585366, 0.8, 1.0, 0.34146341463414637, 0.5, 0.2222222222222222, 0.0, 1.0, 0.6052631578947368]\n",
            "  [0.3137254901960784, 0.5, 0.6956521739130435, 0.5555555555555556, 0.42857142857142855, 0.46153846153846156, 0.3333333333333333, 0.35294117647058826, 0.6363636363636364, 0.4375, 0.6764705882352942]\n",
            "  [0.4358974358974359, 1.0, 0.4084507042253521, 0.8, 0.75, 0.6923076923076923, 0.16666666666666666, 0.4166666666666667, 0.6, 0.38095238095238093, 0.7037037037037037]\n",
            "  [1.0, 0.5, 0.6363636363636364, 0.7272727272727273, 0.6666666666666666, 0.6, 0.0, 0.0, 1.0, 0.6666666666666666, 0.2857142857142857]\n",
            "  [0.4166666666666667, 0.21875, 0.4411764705882353, 0.5714285714285714, 0.75, 0.5625, 0.4, 0.3333333333333333, 0.6666666666666666, 1.0, 0.6428571428571429]\n",
            "  [0.8571428571428571, 0.6666666666666666, 0.7777777777777778, 0.7333333333333333, 0.6428571428571429, 0.6666666666666666, 0.2857142857142857, 0.3157894736842105, 0.4583333333333333, 1.0, 0.6341463414634146]\n",
            "  [0.7272727272727273, 0.0, 0.875, 0.7272727272727273, 0.5714285714285714, 0.3902439024390244, 0.2, 0.2, 0.5, 0.17307692307692307, 0.4461538461538462]\n",
            "  [0.8571428571428571, 0.5384615384615384, 0.5588235294117647, 0.358974358974359, 1.0, 0.631578947368421, 0.0, 0.20833333333333334, 0.3333333333333333, 0.4375, 0.6666666666666666]\n",
            "  [0.7333333333333333, 0.4230769230769231, 0.49206349206349204, 0.875, 1.0, 0.6923076923076923, 0.4, 0.5, 0.5, 0.625, 0.6341463414634146]\n",
            "  [1.0, 0.5, 0.75, 0.7777777777777778, 0.7777777777777778, 0.41379310344827586, 0.36363636363636365, 0.5333333333333333, 0.5333333333333333, 0.4666666666666667, 0.5416666666666666]\n",
            "recall:\n",
            "  [0.0, 0.0, 0.6944444444444444, 0.0, 0.08333333333333333, 0.2727272727272727, 0.0, 0.11764705882352941, 0.8571428571428571, 0.23076923076923078, 0.5454545454545454]\n",
            "  [0.4117647058823529, 0.0, 0.5833333333333334, 0.7333333333333333, 0.16666666666666666, 0.45454545454545453, 0.0, 0.0, 0.2857142857142857, 0.0, 0.6136363636363636]\n",
            "  [0.8823529411764706, 0.0, 0.19444444444444445, 0.6, 0.5833333333333334, 0.5, 0.0, 0.0, 0.14285714285714285, 0.0, 0.6363636363636364]\n",
            "  [0.7058823529411765, 0.6153846153846154, 0.5833333333333334, 0.4, 0.4166666666666667, 0.5909090909090909, 0.0, 0.5882352941176471, 0.07142857142857142, 0.07692307692307693, 0.5909090909090909]\n",
            "  [0.17647058823529413, 0.07692307692307693, 0.05555555555555555, 0.4, 0.25, 0.22727272727272727, 0.0, 0.0, 0.0, 0.0, 0.8863636363636364]\n",
            "  [0.23529411764705882, 0.23076923076923078, 0.7222222222222222, 0.5333333333333333, 0.5833333333333334, 0.2727272727272727, 0.125, 0.4117647058823529, 0.0, 0.15384615384615385, 0.5227272727272727]\n",
            "  [1.0, 0.46153846153846156, 0.4722222222222222, 0.6, 0.4166666666666667, 0.5454545454545454, 0.0, 0.058823529411764705, 0.07142857142857142, 0.07692307692307693, 0.5909090909090909]\n",
            "  [0.0, 0.46153846153846156, 0.19444444444444445, 0.4, 0.5833333333333334, 0.45454545454545453, 0.0, 0.47058823529411764, 0.7142857142857143, 0.38461538461538464, 0.6590909090909091]\n",
            "  [1.0, 0.38461538461538464, 0.6388888888888888, 0.6, 0.5833333333333334, 0.3181818181818182, 0.0, 0.17647058823529413, 0.7857142857142857, 0.15384615384615385, 0.18181818181818182]\n",
            "  [0.9411764705882353, 0.3076923076923077, 0.8611111111111112, 0.3333333333333333, 0.5833333333333334, 0.5, 0.25, 0.17647058823529413, 0.7142857142857143, 0.5384615384615384, 0.5909090909090909]\n",
            "  [0.8823529411764706, 0.6923076923076923, 0.9444444444444444, 0.4666666666666667, 0.6666666666666666, 0.22727272727272727, 0.0625, 0.17647058823529413, 0.14285714285714285, 0.15384615384615385, 0.4772727272727273]\n",
            "  [0.8823529411764706, 0.46153846153846156, 0.5, 0.7333333333333333, 0.5833333333333334, 0.5909090909090909, 0.0, 0.6470588235294118, 0.21428571428571427, 0.5384615384615384, 0.5681818181818182]\n",
            "  [0.5294117647058824, 0.9230769230769231, 0.5833333333333334, 0.26666666666666666, 0.5833333333333334, 0.8181818181818182, 0.1875, 0.17647058823529413, 0.6428571428571429, 0.23076923076923078, 0.5]\n",
            "  [0.29411764705882354, 0.23076923076923078, 0.8333333333333334, 0.6, 0.6666666666666666, 0.45454545454545453, 0.0, 0.0, 0.7142857142857143, 0.07692307692307693, 0.4772727272727273]\n",
            "  [0.5294117647058824, 0.5384615384615384, 0.2777777777777778, 0.5333333333333333, 0.5833333333333334, 0.5, 0.125, 0.4117647058823529, 0.8571428571428571, 0.38461538461538464, 0.4772727272727273]\n",
            "  [0.7058823529411765, 0.38461538461538464, 0.5833333333333334, 0.6, 0.5, 0.4090909090909091, 0.125, 0.5882352941176471, 0.14285714285714285, 0.15384615384615385, 0.4772727272727273]\n",
            "  [0.5294117647058824, 0.5384615384615384, 0.7777777777777778, 0.6666666666666666, 0.5, 0.5454545454545454, 0.375, 0.058823529411764705, 0.8571428571428571, 0.23076923076923078, 0.6818181818181818]\n",
            "  [0.4117647058823529, 0.46153846153846156, 0.3888888888888889, 0.4666666666666667, 0.6666666666666666, 0.3181818181818182, 0.25, 0.5294117647058824, 0.7142857142857143, 0.6153846153846154, 0.38636363636363635]\n",
            "  [0.9411764705882353, 0.15384615384615385, 0.6666666666666666, 0.8, 0.4166666666666667, 0.5454545454545454, 0.125, 0.11764705882352941, 0.35714285714285715, 0.3076923076923077, 0.5227272727272727]\n",
            "  [0.8823529411764706, 0.46153846153846156, 0.3888888888888889, 0.5333333333333333, 0.5, 0.5909090909090909, 0.5, 0.058823529411764705, 0.2857142857142857, 0.7692307692307693, 0.6363636363636364]\n",
            "  [0.4117647058823529, 0.8461538461538461, 0.6666666666666666, 0.5333333333333333, 0.5, 0.6363636363636364, 0.4375, 0.11764705882352941, 0.0, 0.07692307692307693, 0.5227272727272727]\n",
            "  [0.9411764705882353, 0.23076923076923078, 0.4444444444444444, 0.6666666666666666, 0.5, 0.5454545454545454, 0.0625, 0.35294117647058826, 0.5, 0.5384615384615384, 0.5227272727272727]\n",
            "  [1.0, 0.07692307692307693, 0.8055555555555556, 0.5333333333333333, 0.5, 0.4090909090909091, 0.125, 0.29411764705882354, 0.21428571428571427, 0.6153846153846154, 0.4318181818181818]\n",
            "  [0.058823529411764705, 0.23076923076923078, 0.3888888888888889, 0.5333333333333333, 0.5, 0.2727272727272727, 0.0, 0.0, 0.21428571428571427, 0.46153846153846156, 0.9545454545454546]\n",
            "  [0.8823529411764706, 0.5384615384615384, 0.8333333333333334, 0.5333333333333333, 0.5, 0.4090909090909091, 0.125, 0.058823529411764705, 0.2857142857142857, 0.23076923076923078, 0.4090909090909091]\n",
            "  [0.7058823529411765, 0.46153846153846156, 0.19444444444444445, 0.7333333333333333, 0.75, 0.6363636363636364, 0.875, 0.35294117647058826, 0.7857142857142857, 0.3076923076923077, 0.5909090909090909]\n",
            "  [0.47058823529411764, 0.0, 0.19444444444444445, 0.5333333333333333, 0.3333333333333333, 0.7272727272727273, 0.125, 0.11764705882352941, 0.14285714285714285, 0.6923076923076923, 0.6590909090909091]\n",
            "  [0.7058823529411765, 0.5384615384615384, 0.5277777777777778, 0.9333333333333333, 0.5, 0.5454545454545454, 0.0, 0.29411764705882354, 0.5714285714285714, 0.5384615384615384, 0.45454545454545453]\n",
            "  [0.6470588235294118, 0.8461538461538461, 0.8611111111111112, 0.4666666666666667, 0.4166666666666667, 0.4090909090909091, 0.25, 0.29411764705882354, 0.7142857142857143, 0.38461538461538464, 0.5909090909090909]\n",
            "  [0.23529411764705882, 0.5384615384615384, 0.5833333333333334, 0.4666666666666667, 0.5833333333333334, 0.5454545454545454, 0.75, 0.47058823529411764, 0.5714285714285714, 0.5384615384615384, 0.5909090909090909]\n",
            "val_loss:\n",
            "  2.103425979614258\n",
            "  2.1529324054718018\n",
            "  3.134483575820923\n",
            "  1.8389307260513306\n",
            "  2.774247646331787\n",
            "  1.778428554534912\n",
            "  2.2303807735443115\n",
            "  1.7393598556518555\n",
            "  2.2228877544403076\n",
            "  1.6891828775405884\n",
            "  1.8024834394454956\n",
            "  1.8450754880905151\n",
            "  1.8635748624801636\n",
            "  2.033207893371582\n",
            "  2.0099403858184814\n",
            "  2.0997586250305176\n",
            "  1.7889727354049683\n",
            "  2.038301467895508\n",
            "  1.9985121488571167\n",
            "  1.6831270456314087\n",
            "  1.9080055952072144\n",
            "  1.863343358039856\n",
            "  1.8685414791107178\n",
            "  2.4680449962615967\n",
            "  2.177586793899536\n",
            "  1.7737740278244019\n",
            "  2.311854839324951\n",
            "  2.0754315853118896\n",
            "  1.8492380380630493\n",
            "  1.8204622268676758\n",
            "test_acc:\n",
            "  0.30136987566947937\n",
            "  0.36986300349235535\n",
            "  0.35159817337989807\n",
            "  0.47488585114479065\n",
            "  0.31963470578193665\n",
            "  0.4155251085758209\n",
            "  0.456620991230011\n",
            "  0.4292237460613251\n",
            "  0.4337899684906006\n",
            "  0.5525113940238953\n",
            "  0.4840182662010193\n",
            "  0.4885844886302948\n",
            "  0.456620991230011\n",
            "  0.47031962871551514\n",
            "  0.4611872136592865\n",
            "  0.4292237460613251\n",
            "  0.5616438388824463\n",
            "  0.44748857617378235\n",
            "  0.5068492889404297\n",
            "  0.4840182662010193\n",
            "  0.45205479860305786\n",
            "  0.5296803712844849\n",
            "  0.4337899684906006\n",
            "  0.4109589159488678\n",
            "  0.5114155411720276\n",
            "  0.5251141786575317\n",
            "  0.4109589159488678\n",
            "  0.4794520437717438\n",
            "  0.5616438388824463\n",
            "  0.5662100315093994\n",
            "test_loss:\n",
            "  2.0272207260131836\n",
            "  1.962624430656433\n",
            "  2.903184652328491\n",
            "  1.5730137825012207\n",
            "  2.606529951095581\n",
            "  1.699473261833191\n",
            "  1.943938970565796\n",
            "  1.6768300533294678\n",
            "  1.8149094581604004\n",
            "  1.4152865409851074\n",
            "  1.5825401544570923\n",
            "  1.5668822526931763\n",
            "  1.6441878080368042\n",
            "  1.7754387855529785\n",
            "  1.680647373199463\n",
            "  1.9337278604507446\n",
            "  1.4748051166534424\n",
            "  1.7669637203216553\n",
            "  1.6703996658325195\n",
            "  1.5665771961212158\n",
            "  1.769350290298462\n",
            "  1.644492506980896\n",
            "  1.8420312404632568\n",
            "  2.3848953247070312\n",
            "  1.8062124252319336\n",
            "  1.529687523841858\n",
            "  2.0958340167999268\n",
            "  1.833350419998169\n",
            "  1.5887593030929565\n",
            "  1.5431983470916748\n",
            "val_accuracy:\n",
            "  0.26851850748062134\n",
            "  0.3194444477558136\n",
            "  0.2638888955116272\n",
            "  0.3611111044883728\n",
            "  0.24074074625968933\n",
            "  0.3888888955116272\n",
            "  0.33796295523643494\n",
            "  0.42129629850387573\n",
            "  0.35185185074806213\n",
            "  0.48148149251937866\n",
            "  0.4027777910232544\n",
            "  0.4583333432674408\n",
            "  0.3888888955116272\n",
            "  0.37962964177131653\n",
            "  0.3888888955116272\n",
            "  0.38425925374031067\n",
            "  0.45370370149612427\n",
            "  0.39814814925193787\n",
            "  0.39351850748062134\n",
            "  0.46296295523643494\n",
            "  0.39351850748062134\n",
            "  0.5046296119689941\n",
            "  0.4305555522441864\n",
            "  0.37962964177131653\n",
            "  0.42129629850387573\n",
            "  0.4583333432674408\n",
            "  0.35185185074806213\n",
            "  0.43981480598449707\n",
            "  0.5\n",
            "  0.4861111044883728\n",
            "f1_score:\n",
            "  [0.0, 0.0, 0.4347826086956521, 0.0, 0.14285714285714285, 0.37499999999999994, 0.0, 0.15384615384615383, 0.35294117647058826, 0.3529411764705882, 0.4752475247524752]\n",
            "  [0.35, 0.0, 0.34146341463414637, 0.5641025641025641, 0.26666666666666666, 0.5128205128205129, 0.0, 0.0, 0.4, 0.0, 0.5294117647058822]\n",
            "  [0.2727272727272727, 0.0, 0.3111111111111111, 0.6206896551724138, 0.7000000000000001, 0.5238095238095238, 0.0, 0.0, 0.17391304347826086, 0.0, 0.5090909090909091]\n",
            "  [0.7058823529411765, 0.5517241379310345, 0.4158415841584158, 0.48, 0.4761904761904762, 0.5306122448979591, 0.0, 0.42553191489361697, 0.125, 0.13333333333333336, 0.611764705882353]\n",
            "  [0.2727272727272727, 0.07407407407407408, 0.10256410256410256, 0.48, 0.3157894736842105, 0.35714285714285715, 0.0, 0.0, 0.0, 0.0, 0.3577981651376147]\n",
            "  [0.38095238095238093, 0.24000000000000002, 0.46428571428571425, 0.5925925925925926, 0.6666666666666666, 0.3870967741935484, 0.14285714285714288, 0.29166666666666663, 0.0, 0.23529411764705882, 0.49462365591397844]\n",
            "  [0.430379746835443, 0.4000000000000001, 0.5396825396825397, 0.5294117647058824, 0.5, 0.5106382978723404, 0.0, 0.1, 0.125, 0.125, 0.5360824742268041]\n",
            "  [0.0, 0.5454545454545455, 0.27450980392156865, 0.5217391304347827, 0.5599999999999999, 0.47619047619047616, 0.0, 0.27586206896551724, 0.5263157894736842, 0.38461538461538464, 0.49152542372881364]\n",
            "  [0.53125, 0.27777777777777773, 0.575, 0.5294117647058824, 0.6363636363636365, 0.4375, 0.0, 0.24, 0.43999999999999995, 0.18181818181818185, 0.2807017543859649]\n",
            "  [0.7999999999999999, 0.42105263157894735, 0.5794392523364487, 0.47619047619047616, 0.6363636363636365, 0.5789473684210527, 0.34782608695652173, 0.2727272727272727, 0.588235294117647, 0.5, 0.6190476190476191]\n",
            "  [0.5882352941176471, 0.4390243902439025, 0.6181818181818182, 0.5384615384615385, 0.6666666666666666, 0.3333333333333333, 0.1, 0.23076923076923078, 0.25, 0.2666666666666667, 0.5316455696202532]\n",
            "  [0.7894736842105262, 0.42857142857142855, 0.5217391304347826, 0.5789473684210527, 0.7368421052631579, 0.5652173913043478, 0.0, 0.4583333333333333, 0.2857142857142857, 0.5833333333333334, 0.5617977528089888]\n",
            "  [0.5999999999999999, 0.3529411764705882, 0.6086956521739131, 0.4210526315789474, 0.7368421052631579, 0.5538461538461539, 0.2727272727272727, 0.25, 0.6206896551724138, 0.33333333333333337, 0.5866666666666667]\n",
            "  [0.4347826086956522, 0.33333333333333337, 0.47244094488188976, 0.5294117647058824, 0.5, 0.6060606060606061, 0.0, 0.0, 0.5555555555555556, 0.125, 0.5060240963855421]\n",
            "  [0.6923076923076924, 0.5, 0.40816326530612246, 0.5925925925925926, 0.7000000000000001, 0.5, 0.19047619047619047, 0.22950819672131148, 0.41379310344827586, 0.4347826086956522, 0.5185185185185185]\n",
            "  [0.7272727272727272, 0.41666666666666663, 0.4421052631578947, 0.6428571428571429, 0.6, 0.5142857142857142, 0.18181818181818182, 0.29850746268656714, 0.22222222222222224, 0.25, 0.5250000000000001]\n",
            "  [0.6666666666666667, 0.5599999999999999, 0.6588235294117646, 0.689655172413793, 0.6666666666666666, 0.6, 0.38709677419354843, 0.1, 0.631578947368421, 0.3, 0.5714285714285714]\n",
            "  [0.5833333333333334, 0.46153846153846156, 0.509090909090909, 0.56, 0.64, 0.4666666666666667, 0.24242424242424243, 0.29508196721311475, 0.46511627906976755, 0.4, 0.4473684210526316]\n",
            "  [0.4324324324324324, 0.23529411764705882, 0.6153846153846153, 0.6486486486486486, 0.5263157894736842, 0.5333333333333332, 0.18181818181818182, 0.1818181818181818, 0.43478260869565216, 0.3636363636363637, 0.5822784810126581]\n",
            "  [0.6122448979591837, 0.6, 0.56, 0.4571428571428572, 0.631578947368421, 0.5909090909090909, 0.3478260869565218, 0.09523809523809523, 0.38095238095238093, 0.5, 0.6021505376344085]\n",
            "  [0.5384615384615384, 0.3548387096774194, 0.6233766233766234, 0.64, 0.6666666666666666, 0.4444444444444445, 0.4666666666666667, 0.15384615384615383, 0.0, 0.14285714285714288, 0.5609756097560975]\n",
            "  [0.47058823529411764, 0.3157894736842105, 0.5423728813559322, 0.606060606060606, 0.4615384615384615, 0.4999999999999999, 0.10526315789473684, 0.35294117647058826, 0.56, 0.4827586206896552, 0.5897435897435898]\n",
            "  [0.6071428571428571, 0.14285714285714288, 0.5420560747663551, 0.64, 0.6, 0.5142857142857142, 0.14285714285714288, 0.3448275862068966, 0.3157894736842105, 0.47058823529411764, 0.5352112676056339]\n",
            "  [0.1111111111111111, 0.3157894736842105, 0.4827586206896552, 0.6153846153846153, 0.5714285714285715, 0.37499999999999994, 0.0, 0.0, 0.35294117647058826, 0.5454545454545455, 0.4397905759162304]\n",
            "  [0.5660377358490566, 0.3111111111111111, 0.576923076923077, 0.5517241379310344, 0.6, 0.47368421052631576, 0.19047619047619047, 0.1, 0.4, 0.375, 0.5000000000000001]\n",
            "  [0.7741935483870968, 0.5454545454545455, 0.3111111111111111, 0.7333333333333333, 0.6923076923076924, 0.6511627906976744, 0.4307692307692308, 0.33333333333333337, 0.5789473684210527, 0.47058823529411764, 0.611764705882353]\n",
            "  [0.5714285714285714, 0.0, 0.3181818181818182, 0.6153846153846153, 0.4210526315789474, 0.5079365079365079, 0.15384615384615385, 0.14814814814814817, 0.22222222222222224, 0.2769230769230769, 0.5321100917431192]\n",
            "  [0.7741935483870968, 0.5384615384615384, 0.5428571428571428, 0.5185185185185185, 0.6666666666666666, 0.5853658536585366, 0.0, 0.24390243902439027, 0.4210526315789474, 0.4827586206896552, 0.5405405405405405]\n",
            "  [0.6875, 0.5641025641025641, 0.6262626262626263, 0.608695652173913, 0.5882352941176471, 0.5142857142857142, 0.3076923076923077, 0.37037037037037035, 0.588235294117647, 0.4761904761904762, 0.611764705882353]\n",
            "  [0.38095238095238093, 0.5185185185185186, 0.6562499999999999, 0.5833333333333334, 0.6666666666666666, 0.47058823529411764, 0.48979591836734687, 0.5, 0.5517241379310344, 0.5, 0.5652173913043478]\n",
            "mcc:\n",
            "  0.2415771940149857\n",
            "  0.28312555834533665\n",
            "  0.30505433435035284\n",
            "  0.40169563906491895\n",
            "  0.16697101665681469\n",
            "  0.3197717874931093\n",
            "  0.37171310873825686\n",
            "  0.3288393916516501\n",
            "  0.37104538256222985\n",
            "  0.5147658843878625\n",
            "  0.4311959944582344\n",
            "  0.4722163689411193\n",
            "  0.46391288279148485\n",
            "  0.3776491627561417\n",
            "  0.40279258934862705\n",
            "  0.3873214199968105\n",
            "  0.5077961062038482\n",
            "  0.39189240612506426\n",
            "  0.43462443422852537\n",
            "  0.46542757061849244\n",
            "  0.41314836694797713\n",
            "  0.43798261624687707\n",
            "  0.43253212782044065\n",
            "  0.3487473103101098\n",
            "  0.41261525924006787\n",
            "  0.509368338443121\n",
            "  0.3326253543344202\n",
            "  0.451172424526228\n",
            "  0.5136949910040385\n",
            "  0.48867433696219864\n",
            "cohen_kappa:\n",
            "  0.22652281194058732\n",
            "  0.26848713885163955\n",
            "  0.2748344370860927\n",
            "  0.3937571592210768\n",
            "  0.11434637549287241\n",
            "  0.30992928272477993\n",
            "  0.3585146339734958\n",
            "  0.31851869447479686\n",
            "  0.3580380842469705\n",
            "  0.5039578465499976\n",
            "  0.4168192301291044\n",
            "  0.4690471706995575\n",
            "  0.44858135360082063\n",
            "  0.35831111751567124\n",
            "  0.3914271819929138\n",
            "  0.37754618664140216\n",
            "  0.5021655380345051\n",
            "  0.38326947047689386\n",
            "  0.42346746897329823\n",
            "  0.45762949463797575\n",
            "  0.4021884927638545\n",
            "  0.42935579182467487\n",
            "  0.41973030518097953\n",
            "  0.28714507486604235\n",
            "  0.40001417066200595\n",
            "  0.49728714524207007\n",
            "  0.3184165232358003\n",
            "  0.4446409045436567\n",
            "  0.5060072181593694\n",
            "  0.48497248483138145\n",
            "hamming_loss:\n",
            "  0.6666666666666666\n",
            "  0.6255707762557078\n",
            "  0.639269406392694\n",
            "  0.5296803652968036\n",
            "  0.730593607305936\n",
            "  0.5981735159817352\n",
            "  0.5662100456621004\n",
            "  0.5981735159817352\n",
            "  0.5799086757990868\n",
            "  0.4337899543378995\n",
            "  0.5114155251141552\n",
            "  0.4703196347031963\n",
            "  0.4931506849315068\n",
            "  0.5570776255707762\n",
            "  0.547945205479452\n",
            "  0.547945205479452\n",
            "  0.4337899543378995\n",
            "  0.5570776255707762\n",
            "  0.5114155251141552\n",
            "  0.4840182648401826\n",
            "  0.5296803652968036\n",
            "  0.5114155251141552\n",
            "  0.5114155251141552\n",
            "  0.593607305936073\n",
            "  0.5296803652968036\n",
            "  0.4520547945205479\n",
            "  0.6027397260273972\n",
            "  0.4977168949771689\n",
            "  0.4337899543378995\n",
            "  0.45662100456621\n",
            "zero_one_loss:\n",
            "  0.6666666666666667\n",
            "  0.6255707762557078\n",
            "  0.639269406392694\n",
            "  0.5296803652968036\n",
            "  0.730593607305936\n",
            "  0.5981735159817352\n",
            "  0.5662100456621004\n",
            "  0.5981735159817352\n",
            "  0.5799086757990868\n",
            "  0.4337899543378996\n",
            "  0.5114155251141552\n",
            "  0.4703196347031964\n",
            "  0.4931506849315068\n",
            "  0.5570776255707763\n",
            "  0.547945205479452\n",
            "  0.547945205479452\n",
            "  0.4337899543378996\n",
            "  0.5570776255707763\n",
            "  0.5114155251141552\n",
            "  0.4840182648401826\n",
            "  0.5296803652968036\n",
            "  0.5114155251141552\n",
            "  0.5114155251141552\n",
            "  0.5936073059360731\n",
            "  0.5296803652968036\n",
            "  0.452054794520548\n",
            "  0.6027397260273972\n",
            "  0.497716894977169\n",
            "  0.4337899543378996\n",
            "  0.45662100456621\n",
            "auc_roc:\n",
            "  0.7304081772710227\n",
            "  0.7816608477919407\n",
            "  0.7004994382665307\n",
            "  0.8547915050121051\n",
            "  0.7674535470321796\n",
            "  0.853670449341222\n",
            "  0.8131893474803692\n",
            "  0.8573900582727186\n",
            "  0.8442562667557422\n",
            "  0.89539289851276\n",
            "  0.8903051515163308\n",
            "  0.8766313576305315\n",
            "  0.8725416784731075\n",
            "  0.8497998250684726\n",
            "  0.8543510029891302\n",
            "  0.8403049502478901\n",
            "  0.8889341494862223\n",
            "  0.8584169868270265\n",
            "  0.8722597415141965\n",
            "  0.8915104606505525\n",
            "  0.8592638673107977\n",
            "  0.8712835471437093\n",
            "  0.8680158554933803\n",
            "  0.8567507682758869\n",
            "  0.886948492002494\n",
            "  0.9048150213646693\n",
            "  0.8551692560861575\n",
            "  0.8627721255058943\n",
            "  0.8891005751661297\n",
            "  0.8905764491730341\n",
            "brier_score:\n",
            "  0.0725822330430207\n",
            "  0.07070252784202163\n",
            "  0.09190727364887072\n",
            "  0.06201923434466761\n",
            "  0.09313692352304802\n",
            "  0.06665009564868768\n",
            "  0.07026897740153898\n",
            "  0.06786739266422655\n",
            "  0.0702308351565094\n",
            "  0.05272473467104411\n",
            "  0.062207772931308976\n",
            "  0.05908282109071455\n",
            "  0.06418167263819952\n",
            "  0.07157926583539953\n",
            "  0.06734118376669404\n",
            "  0.06925664695028022\n",
            "  0.05639763240213903\n",
            "  0.06838860871428493\n",
            "  0.0653710091952478\n",
            "  0.057971748740060856\n",
            "  0.06566908996747861\n",
            "  0.0666414083895613\n",
            "  0.0642772453800615\n",
            "  0.08282763740026865\n",
            "  0.066577361470821\n",
            "  0.056147198165676176\n",
            "  0.07193675023858583\n",
            "  0.06482064295623681\n",
            "  0.05850079545958524\n",
            "  0.05892396802792999\n",
            "mAP:\n",
            "  0.2999654567249173\n",
            "  0.37724713216764\n",
            "  0.36348098844167753\n",
            "  0.49405445124183844\n",
            "  0.3489543002006568\n",
            "  0.46043721311086333\n",
            "  0.45983236684309625\n",
            "  0.4943280556669185\n",
            "  0.5008999133826016\n",
            "  0.6185864346567904\n",
            "  0.5396291291627763\n",
            "  0.557951583722685\n",
            "  0.5339784267043506\n",
            "  0.5002714278955627\n",
            "  0.5287257485947979\n",
            "  0.5147518351295335\n",
            "  0.5844668768277373\n",
            "  0.5379016112382021\n",
            "  0.5418170671931525\n",
            "  0.5764430485181142\n",
            "  0.5352162442431502\n",
            "  0.5492217071293067\n",
            "  0.5426485517825275\n",
            "  0.5252980725434864\n",
            "  0.5787059727561538\n",
            "  0.6430715038133304\n",
            "  0.5189796134693937\n",
            "  0.5703786361176646\n",
            "  0.6025117678891495\n",
            "  0.5837103436124449\n",
            "f1_beta:\n",
            "  [0.0, 0.0, 0.5605381165919282, 0.0, 0.09999999999999998, 0.30612244897959173, 0.0, 0.12987012987012986, 0.5454545454545454, 0.26785714285714285, 0.5150214592274678]\n",
            "  [0.38461538461538464, 0.0, 0.4545454545454546, 0.6547619047619048, 0.19607843137254904, 0.47619047619047616, 0.0, 0.0, 0.3225806451612903, 0.0, 0.576923076923077]\n",
            "  [0.46583850931677023, 0.0, 0.22875816993464052, 0.6081081081081081, 0.6250000000000002, 0.5092592592592592, 0.0, 0.0, 0.15384615384615385, 0.0, 0.5785123966942148]\n",
            "  [0.7058823529411765, 0.5882352941176471, 0.5023923444976077, 0.4285714285714286, 0.4385964912280702, 0.5652173913043478, 0.0, 0.5102040816326531, 0.08620689655172412, 0.09259259259259259, 0.599078341013825]\n",
            "  [0.20547945205479454, 0.07575757575757577, 0.06802721088435375, 0.4285714285714286, 0.2727272727272727, 0.26595744680851063, 0.0, 0.0, 0.0, 0.0, 0.5571428571428572]\n",
            "  [0.2777777777777778, 0.234375, 0.5909090909090909, 0.5555555555555556, 0.6140350877192983, 0.3092783505154639, 0.13157894736842105, 0.35353535353535354, 0.0, 0.1785714285714286, 0.5111111111111111]\n",
            "  [0.6538461538461539, 0.4347826086956523, 0.49707602339181284, 0.569620253164557, 0.4464285714285715, 0.5309734513274336, 0.0, 0.07042253521126761, 0.08620689655172412, 0.09090909090909091, 0.5676855895196506]\n",
            "  [0.0, 0.49180327868852464, 0.220125786163522, 0.4411764705882354, 0.5737704918032787, 0.4629629629629629, 0.0, 0.3669724770642202, 0.625, 0.3846153846153846, 0.5800000000000001]\n",
            "  [0.7391304347826088, 0.3333333333333333, 0.6117021276595744, 0.569620253164557, 0.603448275862069, 0.3571428571428571, 0.0, 0.19736842105263158, 0.5978260869565217, 0.1639344262295082, 0.21164021164021166]\n",
            "  [0.879120879120879, 0.3448275862068965, 0.7209302325581395, 0.37878787878787873, 0.603448275862069, 0.5288461538461539, 0.2816901408450704, 0.20547945205479454, 0.6578947368421053, 0.5223880597014926, 0.601851851851852]\n",
            "  [0.7352941176470589, 0.5625, 0.7798165137614679, 0.49295774647887325, 0.6666666666666667, 0.2604166666666667, 0.07352941176470588, 0.19480519480519481, 0.17241379310344823, 0.18518518518518517, 0.4976303317535546]\n",
            "  [0.8426966292134831, 0.4477611940298507, 0.5084745762711864, 0.6626506024096386, 0.6363636363636365, 0.5803571428571429, 0.0, 0.5555555555555556, 0.23809523809523808, 0.5555555555555556, 0.5656108597285068]\n",
            "  [0.5555555555555556, 0.5607476635514019, 0.5932203389830508, 0.3125, 0.6363636363636365, 0.6870229007633589, 0.21428571428571427, 0.2, 0.6338028169014085, 0.2631578947368421, 0.5314009661835749]\n",
            "  [0.3378378378378379, 0.2631578947368421, 0.6382978723404256, 0.569620253164557, 0.5882352941176471, 0.505050505050505, 0.0, 0.0, 0.6410256410256411, 0.09090909090909091, 0.4883720930232559]\n",
            "  [0.5844155844155844, 0.5223880597014926, 0.3184713375796179, 0.5555555555555556, 0.6250000000000002, 0.5, 0.14492753623188406, 0.3125, 0.5999999999999999, 0.40322580645161293, 0.4929577464788732]\n",
            "  [0.7142857142857142, 0.39682539682539675, 0.5172413793103449, 0.6164383561643835, 0.5357142857142857, 0.4455445544554456, 0.14285714285714285, 0.42372881355932207, 0.16666666666666666, 0.18181818181818182, 0.49528301886792453]\n",
            "  [0.5769230769230769, 0.546875, 0.7253886010362696, 0.6756756756756757, 0.5555555555555556, 0.5660377358490566, 0.379746835443038, 0.07042253521126761, 0.75, 0.2542372881355932, 0.6329113924050632]\n",
            "  [0.46666666666666656, 0.46153846153846156, 0.42944785276073616, 0.5, 0.6557377049180327, 0.36458333333333326, 0.2469135802469136, 0.4017857142857143, 0.588235294117647, 0.5063291139240507, 0.4086538461538462]\n",
            "  [0.6399999999999999, 0.1785714285714286, 0.6451612903225806, 0.7317073170731707, 0.4545454545454546, 0.5405405405405405, 0.14285714285714285, 0.136986301369863, 0.3846153846153846, 0.3278688524590164, 0.5450236966824644]\n",
            "  [0.75, 0.5084745762711864, 0.4430379746835443, 0.5, 0.5454545454545454, 0.590909090909091, 0.425531914893617, 0.06944444444444445, 0.31746031746031744, 0.6329113924050633, 0.6222222222222222]\n",
            "  [0.45454545454545453, 0.5445544554455446, 0.6486486486486486, 0.5714285714285714, 0.5555555555555556, 0.5426356589147288, 0.44871794871794873, 0.12987012987012986, 0.0, 0.09433962264150944, 0.5373831775700935]\n",
            "  [0.6722689075630252, 0.25862068965517243, 0.47904191616766467, 0.641025641025641, 0.48387096774193544, 0.5263157894736841, 0.07462686567164178, 0.35294117647058826, 0.5223880597014925, 0.5147058823529412, 0.5476190476190477]\n",
            "  [0.7943925233644861, 0.09433962264150944, 0.6744186046511628, 0.5714285714285714, 0.5357142857142857, 0.4455445544554456, 0.13157894736842105, 0.3125, 0.24590163934426226, 0.5479452054794521, 0.4679802955665025]\n",
            "  [0.07246376811594203, 0.25862068965517243, 0.42168674698795183, 0.5633802816901409, 0.5263157894736842, 0.30612244897959173, 0.0, 0.0, 0.2542372881355932, 0.49180327868852464, 0.6501547987616099]\n",
            "  [0.7211538461538463, 0.41666666666666674, 0.7075471698113208, 0.5405405405405406, 0.5357142857142857, 0.4326923076923077, 0.14492753623188406, 0.07042253521126761, 0.3225806451612903, 0.27272727272727276, 0.4411764705882354]\n",
            "  [0.7317073170731708, 0.49180327868852464, 0.22875816993464052, 0.7333333333333332, 0.7258064516129032, 0.6422018348623854, 0.6194690265486726, 0.3448275862068966, 0.6875, 0.3571428571428572, 0.599078341013825]\n",
            "  [0.5063291139240506, 0.0, 0.23026315789473684, 0.5633802816901409, 0.3636363636363636, 0.6201550387596899, 0.13513513513513511, 0.1282051282051282, 0.16666666666666666, 0.43269230769230765, 0.6016597510373444]\n",
            "  [0.7317073170731708, 0.5384615384615384, 0.5337078651685394, 0.7070707070707072, 0.5555555555555556, 0.5607476635514018, 0.0, 0.27173913043478265, 0.5, 0.5147058823529412, 0.48543689320388345]\n",
            "  [0.6626506024096386, 0.7051282051282052, 0.748792270531401, 0.5147058823529411, 0.4716981132075472, 0.4455445544554456, 0.27027027027027023, 0.32051282051282054, 0.6578947368421053, 0.41666666666666663, 0.599078341013825]\n",
            "  [0.2777777777777778, 0.5303030303030303, 0.6104651162790697, 0.5072463768115942, 0.6140350877192983, 0.5128205128205128, 0.6185567010309277, 0.4819277108433735, 0.5633802816901408, 0.5223880597014926, 0.5803571428571429]\n",
            "g_mean:\n",
            "  [0.0, 0.0, 0.4687866253858433, 0.0, 0.2041241452319315, 0.4045199174779452, 0.0, 0.16169041669088863, 0.4364357804719847, 0.41602514716892186, 0.479233838298523]\n",
            "  [0.35400521619692155, 0.0, 0.37523938719322825, 0.5797509043642028, 0.3333333333333333, 0.5170876899950192, 0.0, 0.0, 0.4364357804719847, 0.0, 0.5344701181638046]\n",
            "  [0.37724659112089265, 0.0, 0.3888888888888889, 0.6210590034081188, 0.7144345083117604, 0.5244044240850758, 0.0, 0.0, 0.1781741612749496, 0.0, 0.5195887333176439]\n",
            "  [0.7058823529411765, 0.5547001962252291, 0.43412157106222965, 0.4898979485566356, 0.4811252243246882, 0.5333964609104418, 0.0, 0.44280744277004763, 0.1889822365046136, 0.19611613513818404, 0.6121460921524144]\n",
            "  [0.32539568672798425, 0.07412493166611012, 0.19245008972987526, 0.4898979485566356, 0.32732683535398854, 0.4351941398892446, 0.0, 0.0, 0.0, 0.0, 0.4457215628604321]\n",
            "  [0.48507125007266594, 0.2401922307076307, 0.4970674233862172, 0.5962847939999438, 0.6735753140545634, 0.42640143271122083, 0.14433756729740643, 0.30492478930832245, 0.0, 0.2773500981126146, 0.49534006609204023]\n",
            "  [0.5236349380886428, 0.40360367639778755, 0.5452752542346465, 0.5331139899831832, 0.5103103630798288, 0.5116817192534651, 0.0, 0.14002800840280097, 0.1889822365046136, 0.16012815380508713, 0.5384049882656785]\n",
            "  [0.0, 0.5547001962252291, 0.30123203803835463, 0.5477225575051662, 0.560448538317805, 0.4767312946227962, 0.0, 0.3030216076314281, 0.5455447255899809, 0.38461538461538464, 0.5082247369937667]\n",
            "  [0.6014167670256413, 0.2891574659831201, 0.5778967437740469, 0.5331139899831832, 0.6390096504226938, 0.47193990372426947, 0.0, 0.2572478777137633, 0.4899789435061114, 0.18490006540840973, 0.3344968040028363]\n",
            "  [0.8091547798786778, 0.4529108136578383, 0.6131705234001575, 0.5270462766947299, 0.6390096504226938, 0.5863019699779287, 0.3779644730092272, 0.32539568672798425, 0.5976143046671968, 0.501280411827603, 0.6197506830096351]\n",
            "  [0.6239177481057772, 0.47172817652486326, 0.6587366195483092, 0.5449492609130661, 0.6666666666666666, 0.3768891807222045, 0.125, 0.24253562503633297, 0.3779644730092272, 0.3922322702763681, 0.5351295510095069]\n",
            "  [0.7938841860374447, 0.4296689244236597, 0.5222329678670935, 0.5922200922639821, 0.7637626158259734, 0.565752381856018, 0.0, 0.4791675260559353, 0.3030457633656632, 0.5853694070049635, 0.5618332187193684]\n",
            "  [0.6054055145966812, 0.44877455520405946, 0.6092717958449424, 0.5163977794943222, 0.7637626158259734, 0.5852304798861196, 0.30618621784789724, 0.27500954910846337, 0.6210590034081188, 0.3721042037676254, 0.5956833971812706]\n",
            "  [0.4950737714883372, 0.3721042037676254, 0.5241424183609592, 0.5331139899831832, 0.5163977794943222, 0.642824346533225, 0.0, 0.0, 0.5698028822981898, 0.16012815380508713, 0.5069447770645309]\n",
            "  [0.7276068751089989, 0.501280411827603, 0.46225016352102427, 0.5962847939999438, 0.7144345083117604, 0.5, 0.22360679774997896, 0.25594534844449573, 0.48349377841522817, 0.4385290096535146, 0.5204656769514402]\n",
            "  [0.7276068751089989, 0.4181210050035454, 0.45566118843288356, 0.6445033866354896, 0.6123724356957945, 0.5321811563901744, 0.2041241452319315, 0.3429971702850177, 0.2672612419124244, 0.32025630761017426, 0.5276448530110863]\n",
            "  [0.6902684899626335, 0.560448538317805, 0.6666666666666666, 0.6900655593423543, 0.7071067811865476, 0.6030226891555273, 0.3872983346207417, 0.14002800840280097, 0.6546536707079771, 0.3144854510165755, 0.5790685773616372]\n",
            "  [0.6416889479197478, 0.46153846153846156, 0.5353033790313108, 0.5715476066494082, 0.6405126152203485, 0.5276448530110863, 0.24253562503633297, 0.3290725908572088, 0.4962916669854652, 0.42700841014689905, 0.4530515222556722]\n",
            "  [0.5139940529625884, 0.2773500981126146, 0.6172133998483676, 0.6605782590758164, 0.5455447255899809, 0.5334650693692644, 0.2041241452319315, 0.21693045781865616, 0.445435403187374, 0.36980013081681945, 0.5860942701532693]\n",
            "  [0.6431196942844081, 0.628970902033151, 0.6236095644623235, 0.46188021535170065, 0.6546536707079771, 0.5909090909090909, 0.3651483716701107, 0.12126781251816648, 0.40406101782088427, 0.5337605126836238, 0.6030226891555273]\n",
            "  [0.5659164584181102, 0.43583586846268, 0.6246950475544242, 0.6531972647421809, 0.7071067811865476, 0.46614901074841886, 0.46770717334674267, 0.16169041669088863, 0.0, 0.2773500981126146, 0.5624833862511962]\n",
            "  [0.5433884886490595, 0.3396831102433787, 0.5560384374855327, 0.6085806194501846, 0.4629100498862757, 0.5017452060042544, 0.14433756729740643, 0.35294117647058826, 0.5640760748177662, 0.4853626716970755, 0.594650843494272]\n",
            "  [0.6602252917735247, 0.2773500981126146, 0.5736111347936959, 0.6531972647421809, 0.6123724356957945, 0.5321811563901744, 0.14433756729740643, 0.35007002100700246, 0.35856858280031806, 0.4841820261350419, 0.5512459105263765]\n",
            "  [0.24253562503633297, 0.3396831102433787, 0.49746833816309105, 0.6227991553292184, 0.5773502691896257, 0.4045199174779452, 0.0, 0.0, 0.4629100498862757, 0.5547001962252291, 0.5222329678670935]\n",
            "  [0.6063390625908325, 0.34320323649182205, 0.6063390625908325, 0.5520524474738834, 0.6123724356957945, 0.4797016118001235, 0.22360679774997896, 0.14002800840280097, 0.4364357804719847, 0.4803844614152614, 0.5128225940683708]\n",
            "  [0.7778444682625972, 0.5547001962252291, 0.3888888888888889, 0.7333333333333333, 0.6943650748294137, 0.6513389472789296, 0.5, 0.33384893044479436, 0.600099198148979, 0.5547001962252291, 0.6121460921524144]\n",
            "  [0.5850179393017045, 0.0, 0.4124789556921527, 0.6227991553292184, 0.4364357804719847, 0.5327417265696215, 0.15811388300841897, 0.1533929977694741, 0.2672612419124244, 0.34615384615384615, 0.5422692542049051]\n",
            "  [0.7778444682625972, 0.5384615384615384, 0.543078852951278, 0.5788287614163061, 0.7071067811865476, 0.5869391856534222, 0.0, 0.2475368857441686, 0.4364357804719847, 0.4853626716970755, 0.5504818825631803]\n",
            "  [0.6888467201936644, 0.5983211225424633, 0.6509388146270024, 0.6390096504226938, 0.6454972243679028, 0.5321811563901744, 0.31622776601683794, 0.3834824944236852, 0.5976143046671968, 0.4902903378454601, 0.6121460921524144]\n",
            "  [0.48507125007266594, 0.5188745216627708, 0.6614378277661477, 0.6024640760767093, 0.6735753140545634, 0.47508454947893747, 0.5222329678670935, 0.5009794328681196, 0.5520524474738834, 0.501280411827603, 0.565752381856018]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zea2-lL4nzFX"
      },
      "source": [
        "## Training graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjDuB4jInzFX"
      },
      "outputs": [],
      "source": [
        "# Merge history1 and history2\n",
        "history = {}\n",
        "history['loss'] = history1.history['loss'] + history2.history['loss']\n",
        "history['acc'] = history1.history['acc'] + history2.history['acc']\n",
        "history['val_loss'] = history1.history['val_loss'] + history2.history['val_loss']\n",
        "history['val_acc'] = history1.history['val_acc'] + history2.history['val_acc']\n",
        "history['lr'] = history1.history['lr'] + history2.history['lr']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFjZglv_nzFX"
      },
      "outputs": [],
      "source": [
        "# Plot the training graph\n",
        "def plot_training(history):\n",
        "    acc = history['acc']\n",
        "    val_acc = history['val_acc']\n",
        "    loss = history['loss']\n",
        "    val_loss = history['val_loss']\n",
        "    epochs = range(len(acc))\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15,5))\n",
        "\n",
        "    axes[0].plot(epochs, acc, 'r-', label='Training Accuracy')\n",
        "    axes[0].plot(epochs, val_acc, 'b--', label='Validation Accuracy')\n",
        "    axes[0].set_title('Training and Validation Accuracy')\n",
        "    axes[0].legend(loc='best')\n",
        "\n",
        "    axes[1].plot(epochs, loss, 'r-', label='Training Loss')\n",
        "    axes[1].plot(epochs, val_loss, 'b--', label='Validation Loss')\n",
        "    axes[1].set_title('Training and Validation Loss')\n",
        "    axes[1].legend(loc='best')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "plot_training(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CE8LE_afnzFX"
      },
      "source": [
        "## Evaluate performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGF2zSOrnzFX"
      },
      "outputs": [],
      "source": [
        "# Prediction accuracy on train data\n",
        "score = model.evaluate_generator(train_generator, verbose=1)\n",
        "print(\"Prediction accuracy on train data =\", score[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJ7GcntInzFX"
      },
      "outputs": [],
      "source": [
        "# Prediction accuracy on CV data\n",
        "score = model.evaluate_generator(valid_generator, verbose=1)\n",
        "print(\"Prediction accuracy on CV data =\", score[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GoJEtYhnzFX"
      },
      "outputs": [],
      "source": [
        "# Classification report and confusion matrix\n",
        "from sklearn.metrics import *\n",
        "import seaborn as sns\n",
        "\n",
        "tick_labels = artists_top_name.tolist()\n",
        "\n",
        "def showClassficationReport_Generator(model, valid_generator, STEP_SIZE_VALID):\n",
        "    # Loop on each generator batch and predict\n",
        "    y_pred, y_true = [], []\n",
        "    for i in range(STEP_SIZE_VALID):\n",
        "        (X,y) = next(valid_generator)\n",
        "        y_pred.append(model.predict(X))\n",
        "        y_true.append(y)\n",
        "\n",
        "    # Create a flat list for y_true and y_pred\n",
        "    y_pred = [subresult for result in y_pred for subresult in result]\n",
        "    y_true = [subresult for result in y_true for subresult in result]\n",
        "\n",
        "    # Update Truth vector based on argmax\n",
        "    y_true = np.argmax(y_true, axis=1)\n",
        "    y_true = np.asarray(y_true).ravel()\n",
        "\n",
        "    # Update Prediction vector based on argmax\n",
        "    y_pred = np.argmax(y_pred, axis=1)\n",
        "    y_pred = np.asarray(y_pred).ravel()\n",
        "\n",
        "    # Confusion Matrix\n",
        "    fig, ax = plt.subplots(figsize=(10,10))\n",
        "    conf_matrix = confusion_matrix(y_true, y_pred, labels=np.arange(n_classes))\n",
        "    conf_matrix = conf_matrix/np.sum(conf_matrix, axis=1)\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt=\".2f\", square=True, cbar=False,\n",
        "                cmap=plt.cm.jet, xticklabels=tick_labels, yticklabels=tick_labels,\n",
        "                ax=ax)\n",
        "    ax.set_ylabel('Actual')\n",
        "    ax.set_xlabel('Predicted')\n",
        "    ax.set_title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "    print('Classification Report:')\n",
        "    print(classification_report(y_true, y_pred, labels=np.arange(n_classes), target_names=artists_top_name.tolist()))\n",
        "\n",
        "showClassficationReport_Generator(model, valid_generator, STEP_SIZE_VALID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OwMJrQonzFX"
      },
      "source": [
        "# Evaluate performance by predicting on random images from dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6WFbnfBnzFY"
      },
      "outputs": [],
      "source": [
        "# Prediction\n",
        "from keras.preprocessing import *\n",
        "\n",
        "n = 5\n",
        "fig, axes = plt.subplots(1, n, figsize=(25,10))\n",
        "\n",
        "for i in range(n):\n",
        "    random_artist = random.choice(artists_top_name)\n",
        "    random_image = random.choice(os.listdir(os.path.join(images_dir, random_artist)))\n",
        "    random_image_file = os.path.join(images_dir, random_artist, random_image)\n",
        "\n",
        "    # Original image\n",
        "\n",
        "    test_image = image.load_img(random_image_file, target_size=(train_input_shape[0:2]))\n",
        "\n",
        "    # Predict artist\n",
        "    test_image = image.img_to_array(test_image)\n",
        "    test_image /= 255.\n",
        "    test_image = np.expand_dims(test_image, axis=0)\n",
        "\n",
        "    prediction = model.predict(test_image)\n",
        "    prediction_probability = np.amax(prediction)\n",
        "    prediction_idx = np.argmax(prediction)\n",
        "\n",
        "    labels = train_generator.class_indices\n",
        "    labels = dict((v,k) for k,v in labels.items())\n",
        "\n",
        "    #print(\"Actual artist =\", random_artist.replace('_', ' '))\n",
        "    #print(\"Predicted artist =\", labels[prediction_idx].replace('_', ' '))\n",
        "    #print(\"Prediction probability =\", prediction_probability*100, \"%\")\n",
        "\n",
        "    title = \"Actual artist = {}\\nPredicted artist = {}\\nPrediction probability = {:.2f} %\" \\\n",
        "                .format(random_artist.replace('_', ' '), labels[prediction_idx].replace('_', ' '),\n",
        "                        prediction_probability*100)\n",
        "\n",
        "    # Print image\n",
        "    axes[i].imshow(plt.imread(random_image_file))\n",
        "    axes[i].set_title(title)\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}